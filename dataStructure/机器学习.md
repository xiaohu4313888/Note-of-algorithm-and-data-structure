### GBDT
    1. CART训练弱分类器（层数较4浅），再利用上一轮得到的弱分类器去训练强分类器。
    2. 每一轮的迭代都采用上一轮每个样本损失函数梯度的负梯度进行优化，希望能减小误差(为已有的拟合增加一个偏差)。
        1).根据(x,y)初始化弱分类器。
        2).计算弱分类器损失函数关于分类器的负梯度r。
        3).根据(x,r)拟合另外一个回归树，计算关于负梯度的最优拟合树。
        4）关于负梯度拟合值c，更新新的模型
### XGBoost
    缺失值的内容处理
    XGBoost是GDBT的一种高效实现方法
    XGBoost是一种贪心算法，会先分裂到最大，在回头剪枝处理，避免了陷入局部最优值。

### lightGBM
    解决的是内存无法一次读取全部数据的问题,基于预排序的算法，需要内存量往往是完整数据量的两倍。
    lightGBM是基于Histogram的决策树算法。
    带深度限制的leaf_wise的叶子生长策略。
#### lightGBM中的直方图优化算法
    在树分裂计算分裂特征的增益时，xgboost 采用了预排序的方法来处理节点分裂，这样计算的分裂点比较精确。但是，也造成了很大的时间开销。为了解决这个问题，Lightgbm 选择了基于 histogram 的决策树算法。
    histogram算法简单来说，就是先对特征值进行装箱处理，形成一个一个的bins。对于连续特征来说，装箱处理就是特征工程中的离散化：如[0,0.3)—>0，[0.3,0.7)—->1等。在Lightgbm中默认的#bins为256（1个字节的能表示的长度，可以设置）。
    对于分类特征来说，则是每一种取值放入一个bin，且当取值的个数大于max bin数时，会忽略那些很少出现的category值。
    对每个特征创建一个直方图，每个直方图包括X个bin,直方图包括两类信息，每个bin中的样梯度之和和bin中样本的数量。
    进行特征选择时候，只需要根据直方图的离散值，遍历寻找最优的分割。
    lightGBM利用正则化后的映射数据代替了原数据，bin的数量决定了惩罚的强度，bin越少惩罚越严重，欠拟合风险越大。
    level-wise和leaf-wise两种生长策略
